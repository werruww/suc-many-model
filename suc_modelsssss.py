# -*- coding: utf-8 -*-
"""suc_modelsssss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1as7y3sBHgi-zYROCGLwiR-NEvZRChY_C
"""







from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('google/byt5-small')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')

# text input
text    = 'English to French: How much is it?'
inputs  = tokenizer(text, padding='longest', return_tensors='pt')

outputs = model.generate(**inputs)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)



from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('google/byt5-small')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')

# text input
text    = 'English to French: How much is it?'
inputs  = tokenizer(text, padding='longest', return_tensors='pt')

# Generate with a maximum length
max_tokens_to_generate = 50  # You can adjust this value
outputs = model.generate(**inputs, max_length=max_tokens_to_generate)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)

Who is Python?

from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('google/byt5-small')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')

# text input
text    = 'Who is Python?'
inputs  = tokenizer(text, padding='longest', return_tensors='pt')

# Generate with a maximum length
max_tokens_to_generate = 50  # You can adjust this value
outputs = model.generate(**inputs, max_length=max_tokens_to_generate)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)



from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('google/byt5-large')
tokenizer = AutoTokenizer.from_pretrained('google/byt5-large')

# text input
text    = 'Who is Python?'
inputs  = tokenizer(text, padding='longest', return_tensors='pt')

# Generate with a maximum length
max_tokens_to_generate = 50  # You can adjust this value
outputs = model.generate(**inputs, max_length=max_tokens_to_generate)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)

from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('openai-community/gpt2')
tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')

# text input
text    = 'Who is Python?'
inputs  = tokenizer(text, padding='longest', return_tensors='pt')

# Generate with a maximum length
max_tokens_to_generate = 50  # You can adjust this value
outputs = model.generate(**inputs, max_length=max_tokens_to_generate)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)

from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('openai-community/gpt2')
tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')



# text input
text    = 'Who is Python?'
inputs  = tokenizer(text)

# Generate with a maximum length
#max_tokens_to_generate = 512  # You can adjust this value
outputs = model.generate(**inputs)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)



from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch

model     = T5ForConditionalGeneration.from_pretrained('openai-community/gpt2')
tokenizer = AutoTokenizer.from_pretrained('openai-community/gpt2')



# text input
text    = 'Who is Python?'
# Ensure tokenizer returns PyTorch tensors
inputs  = tokenizer(text, return_tensors='pt')

# Generate with a maximum length
#max_tokens_to_generate = 512  # You can adjust this value
outputs = model.generate(**inputs)
output  = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output)

!pip install hf_xet

from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = TFGPT2Model.from_pretrained('gpt2')
text = "hi"
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
print(output)

from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = "hi"
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
print(output)

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

from transformers import pipeline, set_seed
import torch
generator = pipeline('text-generation', model='google/byt5-large', device_map="auto")
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=1)

from transformers import pipeline, set_seed
import torch
generator = pipeline('text-generation', model='google/byt5-large', device_map="auto")
set_seed(42)
generator("how are you?", max_length=512, num_return_sequences=1)

Rephrase the sentence "I love AI models."

from transformers import pipeline, set_seed
import torch
generator = pipeline('text-generation', model='google/byt5-large', device_map="auto")
set_seed(42)
generator("Rephrase the sentence 'I love AI models.' ", max_length=512, num_return_sequences=1)

translation'

from transformers import pipeline, set_seed
import torch
generator = pipeline('translation', model='google/byt5-large', device_map="auto")
set_seed(42)
generator("translate into arabic the sentence 'I love AI models.' ", max_length=512, num_return_sequences=1)

# pip install accelerate
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer  = T5Tokenizer.from_pretrained("t5-small")
model      = T5ForConditionalGeneration.from_pretrained("t5-small", device_map="cpu")

input_text = "hi"

input_ids  = tokenizer(input_text, return_tensors="pt").input_ids.to("cpu")

outputs    = model.generate(input_ids, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import pipeline
unmasker = pipeline('fill-mask', model='bert-base-uncased')

for i in unmasker("AI will [MASK] the world."):
	print(i)

from transformers import pipeline
unmasker = pipeline('fill-mask', model='geoogle/byt5-larg')

for i in unmasker("AI will [MASK] the world."):
	print(i)

from transformers import pipeline
unmasker = pipeline('fill-mask', model='gpt2')

for i in unmasker("AI will [MASK] the world."):
	print(i)

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pandas as pd
import os

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Dataset
lines = open('fra.txt', 'r', encoding='utf-8').read().split('\n')

inputs, outputs = [], []

for k, line in enumerate(lines):
	inp, out, _ = line.split('\t')
	inputs.append(inp.lower())
	outputs.append(out.lower())

	if k > 1024 * 8:
		break

df = pd.DataFrame({'input' : inputs, 'target' : outputs})

class TranslationDatset(Dataset):
	def __init__(self, df, tokenizer, input_maxlen, target_maxlen):
		self.tokenizer     = tokenizer
		self.df            = df
		self.input_maxlen  = input_maxlen
		self.target_maxlen = target_maxlen

	def __len__(self):
		return len(self.df)

	def __getitem__(self, index):
		inputs  = self.df.iloc[index]['input']
		targets = self.df.loc[index]['target']

		inputs_tokens = self.tokenizer.encode_plus(inputs,
								max_length=self.input_maxlen,
								truncation=True,
								padding='max_length',
								return_tensors='pt')


		targets_tokens = self.tokenizer.encode_plus(targets,
								max_length=self.target_maxlen,
								truncation=True,
								padding='max_length',
								return_tensors='pt')

		return {'input_ids'              : inputs_tokens['input_ids'].squeeze(),
				'attention_mask'         : inputs_tokens['attention_mask'].squeeze(),
				'decoder_input_ids'      : targets_tokens['input_ids'].squeeze()[:-1],
				'decoder_attention_mask' : targets_tokens['attention_mask'].squeeze()[:-1],
				'labels'                 : targets_tokens['input_ids'].squeeze()[1:]}




# Model
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model     = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)

dataset = TranslationDatset(df, tokenizer, 128, 128)
dataset = DataLoader(dataset, batch_size=4)

# Hyperparameters
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)

models_dir = 'models'
os.makedirs(models_dir, exist_ok=True)

def save_model(model, epoch):
	model_path = os.path.join(models_dir, f'model_epoch_{epoch}.pt')
	torch.save(model.state_dict(), model_path)

def eval_model():
	model.eval()
	for example in df.sample(4).itertuples():
		inputs      = example.input
		input_ids   = tokenizer.encode(inputs, return_tensors='pt').to(device)
		targets_ids = model.generate(input_ids=input_ids, max_length=128, num_beams=3, repetition_penalty=2.5, length_penalty=1.0)
		output      = tokenizer.decode(targets_ids[0], skip_special_tokens=True)

		print(f"Text: '{inputs}' -> Translation: '{output}'  Target: '{example.target}'")

for epoch in range(1024):
	epoch_loss = 0.0
	with tqdm(dataset, desc=f'Epoch {epoch + 1}', unit='batch', leave=False) as tepoch:
		for i, batch in enumerate(tepoch):
			input_ids              = batch['input_ids'].to(device)
			attention_mask         = batch['attention_mask'].to(device)
			decoder_input_ids      = batch['decoder_input_ids'].to(device)
			decoder_attention_mask = batch['decoder_attention_mask'].to(device)
			labels                 = batch['labels'].to(device)

			outputs = model(input_ids=input_ids, attention_mask=attention_mask,
						decoder_input_ids=decoder_input_ids,decoder_attention_mask=decoder_attention_mask,
						labels=labels)

			loss = outputs[0]
			optimizer.zero_grad()
			loss.backward() # backpropagation
			optimizer.step()

			epoch_loss += loss.item()
			tepoch.set_postfix(losss=epoch_loss/(i + 1))

		save_model(model, epoch + 1)
		eval_model()



import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pandas as pd
import os

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Dataset
# Make sure 'fra.txt' exists and is in the correct directory
try:
    lines = open('fra.txt', 'r', encoding='utf-8').read().split('\n')
except FileNotFoundError:
    print("Error: 'fra.txt' not found. Please ensure the dataset file is in the correct location.")
    exit() # Exit or handle the error appropriately

inputs, outputs = [], []

for k, line in enumerate(lines):
    try:
        inp, out, _ = line.split('\t')
        inputs.append(inp.lower())
        outputs.append(out.lower())
    except ValueError:
        # Handle lines that don't have the expected number of columns
        print(f"Skipping line {k} due to unexpected format: {line}")
        continue

    if k > 1024 * 8:
        break

df = pd.DataFrame({'input' : inputs, 'target' : outputs})

class TranslationDatset(Dataset):
    def __init__(self, df, tokenizer, input_maxlen, target_maxlen):
        self.tokenizer     = tokenizer
        self.df            = df
        self.input_maxlen  = input_maxlen
        self.target_maxlen = target_maxlen

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        inputs  = self.df.iloc[index]['input']
        targets = self.df.loc[index]['target']

        inputs_tokens = self.tokenizer.encode_plus(inputs,
                                max_length=self.input_maxlen,
                                truncation=True,
                                padding='max_length',
                                return_tensors='pt')


        targets_tokens = self.tokenizer.encode_plus(targets,
                                max_length=self.target_maxlen,
                                truncation=True,
                                padding='max_length',
                                return_tensors='pt')

        return {'input_ids'              : inputs_tokens['input_ids'].squeeze(),
                'attention_mask'         : inputs_tokens['attention_mask'].squeeze(),
                'decoder_input_ids'      : targets_tokens['input_ids'].squeeze()[:-1],
                'decoder_attention_mask' : targets_tokens['attention_mask'].squeeze()[:-1],
                'labels'                 : targets_tokens['input_ids'].squeeze()[1:]}




# Model
# Added local_files_only=False to force redownload of model files
tokenizer = T5Tokenizer.from_pretrained('google/byt5-large', local_files_only=False)
model     = T5ForConditionalGeneration.from_pretrained('google/byt5-large').to(device)

dataset = TranslationDatset(df, tokenizer, 128, 128)
dataset = DataLoader(dataset, batch_size=4)

# Hyperparameters
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)

models_dir = 'models'
os.makedirs(models_dir, exist_ok=True)

def save_model(model, epoch):
    model_path = os.path.join(models_dir, f'model_epoch_{epoch}.pt')
    torch.save(model.state_dict(), model_path)

def eval_model():
    model.eval()
    for example in df.sample(4).itertuples():
        inputs      = example.input
        input_ids   = tokenizer.encode(inputs, return_tensors='pt').to(device)
        targets_ids = model.generate(input_ids=input_ids, max_length=128, num_beams=3, repetition_penalty=2.5, length_penalty=1.0)
        output      = tokenizer.decode(targets_ids[0], skip_special_tokens=True)

        print(f"Text: '{inputs}' -> Translation: '{output}'  Target: '{example.target}'")

for epoch in range(1024):
    epoch_loss = 0.0
    with tqdm(dataset, desc=f'Epoch {epoch + 1}', unit='batch', leave=False) as tepoch:
        for i, batch in enumerate(tepoch):
            input_ids              = batch['input_ids'].to(device)
            attention_mask         = batch['attention_mask'].to(device)
            decoder_input_ids      = batch['decoder_input_ids'].to(device)
            decoder_attention_mask = batch['decoder_attention_mask'].to(device)
            labels                 = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                        decoder_input_ids=decoder_input_ids,decoder_attention_mask=decoder_attention_mask,
                        labels=labels)

            loss = outputs[0]
            optimizer.zero_grad()
            loss.backward() # backpropagation
            optimizer.step()

            epoch_loss += loss.item()
            tepoch.set_postfix(losss=epoch_loss/(i + 1))

        save_model(model, epoch + 1)
        eval_model()



import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import pandas as pd
import os
from huggingface_hub import snapshot_download

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Dataset
# Make sure 'fra.txt' exists and is in the correct directory
try:
    lines = open('fra.txt', 'r', encoding='utf-8').read().split('\n')
except FileNotFoundError:
    print("Error: 'fra.txt' not found. Please ensure the dataset file is in the correct location.")
    exit() # Exit or handle the error appropriately

inputs, outputs = [], []

for k, line in enumerate(lines):
    try:
        inp, out, _ = line.split('\t')
        inputs.append(inp.lower())
        outputs.append(out.lower())
    except ValueError:
        # Handle lines that don't have the expected number of columns
        print(f"Skipping line {k} due to unexpected format: {line}")
        continue

    if k > 1024 * 8:
        break

df = pd.DataFrame({'input' : inputs, 'target' : outputs})

class TranslationDatset(Dataset):
    def __init__(self, df, tokenizer, input_maxlen, target_maxlen):
        self.tokenizer     = tokenizer
        self.df            = df
        self.input_maxlen  = input_maxlen
        self.target_maxlen = target_maxlen

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        inputs  = self.df.iloc[index]['input']
        targets = self.df.loc[index]['target']

        inputs_tokens = self.tokenizer.encode_plus(inputs,
                                max_length=self.input_maxlen,
                                truncation=True,
                                padding='max_length',
                                return_tensors='pt')


        targets_tokens = self.tokenizer.encode_plus(targets,
                                max_length=self.target_maxlen,
                                truncation=True,
                                padding='max_length',
                                return_tensors='pt')

        return {'input_ids'              : inputs_tokens['input_ids'].squeeze(),
                'attention_mask'         : inputs_tokens['attention_mask'].squeeze(),
                'decoder_input_ids'      : targets_tokens['input_ids'].squeeze()[:-1],
                'decoder_attention_mask' : targets_tokens['decoder_attention_mask'].squeeze()[:-1],
                'labels'                 : targets_tokens['input_ids'].squeeze()[1:]}




# Model
# Download model files explicitly
model_name = 'google/byt5-large'
cache_dir = snapshot_download(model_name)

# Load tokenizer and model from the downloaded directory
tokenizer = T5Tokenizer.from_pretrained(cache_dir)
model     = T5ForConditionalGeneration.from_pretrained(cache_dir).to(device)


dataset = TranslationDatset(df, tokenizer, 128, 128)
dataset = DataLoader(dataset, batch_size=4)

# Hyperparameters
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)

models_dir = 'models'
os.makedirs(models_dir, exist_ok=True)

def save_model(model, epoch):
    model_path = os.path.join(models_dir, f'model_epoch_{epoch}.pt')
    torch.save(model.state_dict(), model_path)

def eval_model():
    model.eval()
    for example in df.sample(4).itertuples():
        inputs      = example.input
        input_ids   = tokenizer.encode(inputs, return_tensors='pt').to(device)
        targets_ids = model.generate(input_ids=input_ids, max_length=128, num_beams=3, repetition_penalty=2.5, length_penalty=1.0)
        output      = tokenizer.decode(targets_ids[0], skip_special_tokens=True)

        print(f"Text: '{inputs}' -> Translation: '{output}'  Target: '{example.target}'")

for epoch in range(1024):
    epoch_loss = 0.0
    with tqdm(dataset, desc=f'Epoch {epoch + 1}', unit='batch', leave=False) as tepoch:
        for i, batch in enumerate(tepoch):
            input_ids              = batch['input_ids'].to(device)
            attention_mask         = batch['attention_mask'].to(device)
            decoder_input_ids      = batch['decoder_input_ids'].to(device)
            decoder_attention_mask = batch['decoder_attention_mask'].to(device)
            labels                 = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                        decoder_input_ids=decoder_input_ids,decoder_attention_mask=decoder_attention_mask,
                        labels=labels)

            loss = outputs[0]
            optimizer.zero_grad()
            loss.backward() # backpropagation
            optimizer.step()

            epoch_loss += loss.item()
            tepoch.set_postfix(losss=epoch_loss/(i + 1))

        save_model(model, epoch + 1)
        eval_model()

!git clone https://github.com/google-research/byt5.git

!pip install t5

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/byt5/byt5

!pip install multilingual_t5

!pip install multilingual

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/byt5
!python /content/byt5/byt5/tasks_test.py

import sys
import os

# Get the current working directory (where you cloned the byt5 repository)
current_dir = os.getcwd()
# Add the parent directory of byt5 to sys.path
sys.path.append(current_dir)

!python /content/byt5/byt5/tasks_test.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/byt5
!python -m byt5.tasks_test



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/byt5
!python -m byt5.tasks_test

!t5_mesh_transformer--module_import="multilingual_t5.tasks"

!pip install -q t5

!python /content/byt5/byt5/tasks.py



"""https://github.com/tensorflow/mesh/tree/master"""

!pip install mesh-tensorflow

import mesh_tensorflow.auto_mtf

!python /content/byt5/byt5/get_length_stats.py

import mesh_tensorflow.auto_mtf

graph = mtf.Graph()
mesh = mtf.Mesh(graph, "my_mesh")
# Insert model code here.
outputs = [logits, loss]  # iterable of mtf.Tensor, the outputs you're computing
mesh_shape = [("processor_rows", 2), ("processor_cols", 2)]
layout_rules = mtf.auto_mtf.layout(graph, mesh_shape, outputs)



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/byt5
!python -m byt5.tasks_test

!pip uninstall tensorflow -y
!pip install tensorflow==2.7
!pip install mesh-tensorflow
import mesh_tensorflow.auto_mtf

>>> import sentencepiece as spm
>>> s = spm.SentencePieceProcessor(model_file='spm.model')
>>> for n in range(5):
...     s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)
...
['▁', 'N', 'e', 'w', '▁York']
['▁', 'New', '▁York']
['▁', 'New', '▁Y', 'o', 'r', 'k']
['▁', 'New', '▁York']
['▁', 'New', '▁York']

!pip install sentencepiece

import sentencepiece as spm
s = spm.SentencePieceProcessor(model_file='spm.model')
for n in range(5):
    s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)

# Commented out IPython magic to ensure Python compatibility.
# Change to the directory where the spm.model file might be located
# Adjust this path based on the actual location of 'spm.model'
# %cd /content/byt5/byt5

import sentencepiece as spm
import os

# Attempt to load the model file from the new current directory
try:
    s = spm.SentencePieceProcessor(model_file='spm.model')
    for n in range(5):
        print(s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1))
except OSError as e:
    print(f"Could not load 'spm.model'. Ensure it exists in the current directory: {os.getcwd()}")
    print(f"Error details: {e}")

# ${HOME}/dir1/user_dir/tasks.py

import functools
import seqio
import tensorflow_datasets as tfds
from t5.evaluation import metrics
from t5.data import preprocessors

vocabulary = seqio.SentencePieceVocabulary(
    'gs://t5-data/vocabs/cc_all.32000/sentencepiece.model', extra_ids=100)
output_features = {
    'inputs': seqio.Feature(vocabulary=vocabulary),
    'targets': seqio.Feature(vocabulary=vocabulary)
}

seqio.TaskRegistry.add(
    'wmt_t2t_de_en_v003',
    source=seqio.TfdsDataSource(tfds_name='wmt_t2t_translate/de-en:1.0.0'),
    preprocessors=[
        functools.partial(
            preprocessors.translate,
            source_language='de', target_language='en'),
        seqio.preprocessors.tokenize,
        seqio.CacheDatasetPlaceholder(),
        seqio.preprocessors.append_eos_after_trim,
    ],
    metric_fns=[metrics.bleu],
    output_features=output_features)



!pip install tensorflow

!pip install tensorflow==2.7

!% sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev

!pip install flax jax jaxlib



"""https://github.com/patil-suraj/bloom-t5x/blob/main/run_bloom.py"""

!python /content/modeling_flax_bloom.py

!git clone https://github.com/patil-suraj/bloom-t5x.git

# Commented out IPython magic to ensure Python compatibility.
# %cd bloom-t5x
!python modeling_flax_bloom.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/bloom-t5x
!python modeling_flax_bloom.py

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/bloom-t5x
!python -m bloom_t5x.modeling_flax_bloom

# Commented out IPython magic to ensure Python compatibility.
import sys
import os

# Get the parent directory of bloom-t5x, which is /content
parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
# Add /content to sys.path
sys.path.append(parent_dir)

# Now try running the script as a module from the /content directory
# %cd /content
!python -m bloom_t5x.modeling_flax_bloom

# Commented out IPython magic to ensure Python compatibility.
import sys
import os

# Add the directory containing the bloom_t5x package to the system path
sys.path.append('/content')

# Change to the directory where you want to execute from,
# which is often the root of the package for module execution.
# In this case, it's likely still /content since bloom_t5x is under it.
# %cd /content

# Now try running the script as a module
!python -m bloom_t5x.modeling_flax_bloom

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/bloom-t5x
!python modeling_flax_bloom.py

# Commented out IPython magic to ensure Python compatibility.
import sys
import os

# Add the directory containing the script and its dependencies to the system path
# This makes 'configuration_bloom' and 'layers' discoverable for the script
sys.path.append('/content/bloom-t5x')

# Now, you can import the script's components or potentially run it,
# but since it's a script with top-level execution logic,
# we'll need to run it as if it were a standalone script,
# but with the knowledge that its directory is in sys.path.

# One way to execute a script after adding its directory to sys.path
# is to use the `runpy` module, but that might be overly complex.

# Let's try running it directly again, but after ensuring the path is set.
# This might work now that the system knows to look in this directory for modules.
# %cd /content/bloom-t5x
!python modeling_flax_bloom.py

!python /content/bloom-t5x/run_bloom.py

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1")

# حدد النص الذي تريد الإكمال منه
input_text = "hi"

# حول النص إلى معرفات رمزية (tensors)
# return_tensors='pt' يضمن أن الناتج هو موتر PyTorch
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# استخدم طريقة generate للإكمال
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch # استيراد مكتبة torch لمعالجة الموترات

tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-1b1")

# حدد النص الذي تريد الإكمال منه
input_text = "مرحبا، أنا نموذج لغوي،"

# حول النص إلى معرفات رمزية (tensors)
# return_tensors='pt' يضمن أن الناتج هو موتر PyTorch
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# استخدم طريقة generate للإكمال
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)



# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch # استيراد مكتبة torch لمعالجة الموترات

tokenizer = AutoTokenizer.from_pretrained("bigscience/tokenizer")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-3b-intermediate", device_map="auto")

# حدد النص الذي تريد الإكمال منه
input_text = "hi"

# حول النص إلى معرفات رمزية (tensors)
# return_tensors='pt' يضمن أن الناتج هو موتر PyTorch
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# استخدم طريقة generate للإكمال
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
outputs = model.generate(input_ids, max_length=50, num_return_sequences=1)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch # استيراد مكتبة torch لمعالجة الموترات

tokenizer = AutoTokenizer.from_pretrained("bigscience/tokenizer")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-3b-intermediate", device_map="auto")

# حدد النص الذي تريد الإكمال منه
input_text = "Who is python?"

# حول النص إلى معرفات رمزية (tensors)
# return_tensors='pt' يضمن أن الناتج هو موتر PyTorch
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# استخدم طريقة generate للإكمال
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=True)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)



# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch # استيراد مكتبة torch لمعالجة الموترات

tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-7b1", device_map="auto")

# حدد النص الذي تريد الإكمال منه
input_text = "Who is python?"

# حول النص إلى معرفات رمزية (tensors)
# return_tensors='pt' يضمن أن الناتج هو موتر PyTorch
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# استخدم طريقة generate للإكمال
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=True)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)

"""https://huggingface.co/docs/transformers/main/en/model_doc/t5"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-7b1", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")
input_ids = tokenizer.encode("ai is", return_tensors="pt")
outs = model.generate(input_ids)
output = tokenizer.decode(outs.squeeze())

print(output)









from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-7b1", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")
input_ids = tokenizer.encode("ai is", return_tensors="pt")
outs = model.generate(input_ids)
output = tokenizer.decode(outs.squeeze())
print(output)





import requests
from PIL import Image
from transformers import BlipProcessor, Blip2ForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip2-flan-t5-xxl")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-flan-t5-xxl")

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

question = "how many dogs are in the picture?"
inputs = processor(raw_image, question, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Salesforce/xLAM-2-3b-fc-r"
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto"
).to(device)

prompt = "What is Salesforce?"
messages = [{"role": "user", "content": prompt}]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

generated = model.generate(inputs, max_new_tokens=128)
output = tokenizer.decode(
    generated[0],
    skip_special_tokens=True,
)
print(output)

!rm -rf /root/.cache

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Salesforce/xLAM-2-3b-fc-r"
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto"
).to(device)

prompt = "Who is python?"
messages = [{"role": "user", "content": prompt}]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

generated = model.generate(inputs, max_new_tokens=128)
output = tokenizer.decode(
    generated[0],
    skip_special_tokens=True,
)
print(output)

import torch
from transformers import pipeline

pipeline = pipeline(
    task="text2text-generation",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("translate English to French: The weather is nice today.")

import torch
from transformers import pipeline

pipeline = pipeline(
    task="text2text-generation",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("translate English to Arabic: The weather is nice today.")



import torch
from transformers import pipeline

pipeline = pipeline(
    task="text2text-generation",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("translate English to French: Request TPU quota as required. GCP projects come with 8 cores by default, which is enough to run one training experiment on a single TPU host. If you want to run multi-host training or run multiple trials in parallel, you will need more quota. Navigate to.")

import torch
from transformers import pipeline

pipeline = pipeline(
    task="text2text-generation",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("translate English to French: Request TPU quota as required. GCP projects come with 8 cores by default, which is enough to run one training experiment on a single TPU host. If you want to run multi-host training or run multiple trials in parallel, you will need more quota. Navigate to.")

import torch
from transformers import pipeline

pipeline = pipeline(
    task="summarization",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    device=0
)
pipeline("Request TPU quota as required. GCP projects come with 8 cores by default, which is enough to run one training experiment on a single TPU host. If you want to run multi-host training or run multiple trials in parallel, you will need more quota. Navigate to.")



import torch
from transformers import pipeline

pipeline = pipeline(
    task="summarization",
    model="google-t5/t5-base",
    torch_dtype=torch.float16,
    max_length=33,
    device=0
)
pipeline("Request TPU quota as required. GCP projects come with 8 cores by default, which is enough to run one training experiment on a single TPU host. If you want to run multi-host training or run multiple trials in parallel, you will need more quota. Navigate to.")

import torch
from transformers import pipeline, Conversation

# Load a conversational model from Google (like Meena or LaMDA)
# Please note: Directly using these models through transformers might require specific configurations or versions.
# Replace 'google/some-conversational-model' with the actual model identifier if available in transformers.
# As of my last update, Meena and LaMDA might not be directly available for easy use in the transformers library.
# You might need to use a different Google model available in transformers that is suitable for conversation.
# Example using a T5 model configured for conversation (less ideal but possible):
# model_name = "google-t5/t5-base"
# Instead, let's use a model known for chat capabilities if available.
# As of my last update, models like DialoGPT or BlenderBot are more readily available for conversational tasks
# in the transformers library, but are not from Google.

# For a Google model, you might consider models like "google/flan-t5-base" and adapt it for chat-like interactions,
# or look for newer Google models released and integrated into the transformers library.
# Let's use a general T5 model as an example, noting it's not specifically a chat model.
model_name = "google-t5/t5-base"

# Create a conversational pipeline
# Note: T5 models are primarily for text2text tasks. For better chat, a dedicated conversational model is preferred.
# Using 'text2text-generation' with T5 for a chat-like interaction might require careful prompt engineering.
pipeline = pipeline(
    task="text2text-generation", # Or 'conversational' if a suitable model and task are available
    model=model_name,
    torch_dtype=torch.float16,
    device=0 # Use GPU if available
)

# Define the initial conversation
# For T5, the input needs to be framed as a text2text task, e.g., answering a question.
# A true conversational pipeline would handle turns automatically.
initial_input = "Answer: How are you today?"

# Use the pipeline to generate a response
# If the pipeline task was 'conversational', you would use a Conversation object.
# conversation = Conversation("Hello, how are you?")
# conversation = pipeline(conversation)
# print(conversation.generated_responses[-1])

# For the text2text-generation task with T5:
output = pipeline(initial_input)
print(output)

# To simulate turns with T5, you would need to manually manage the conversation history and prompt.
# Example of a second turn (manual for text2text):
# second_input = f"Answer the following question based on this conversation: {initial_input} {output[0]['generated_text']} What did I ask you first?"
# second_output = pipeline(second_input)
# print(second_output)

# If a dedicated conversational model from Google becomes available in transformers with a 'conversational' task,
# the approach with the Conversation object would be more appropriate.

import torch
from transformers import pipeline

# Load a conversational model from Google (like Meena or LaMDA)
# Please note: Directly using these models through transformers might require specific configurations or versions.
# Replace 'google/some-conversational-model' with the actual model identifier if available in transformers.
# As of my last update, Meena and LaMDA might not be directly available for easy use in the transformers library.
# You might need to use a different Google model available in transformers that is suitable for conversation.
# Example using a T5 model configured for conversation (less ideal but possible):
# model_name = "google-t5/t5-base"
# Instead, let's use a model known for chat capabilities if available.
# As of my last update, models like DialoGPT or BlenderBot are more readily available for conversational tasks
# in the transformers library, but are not from Google.

# For a Google model, you might consider models like "google/flan-t5-base" and adapt it for chat-like interactions,
# or look for newer Google models released and integrated into the transformers library.
# Let's use a general T5 model as an example, noting it's not specifically a chat model.
model_name = "google-t5/t5-base"

# Create a conversational pipeline
# Note: T5 models are primarily for text2text tasks. For better chat, a dedicated conversational model is preferred.
# Using 'text2text-generation' with T5 for a chat-like interaction might require careful prompt engineering.
pipeline = pipeline(
    task="text2text-generation", # Or 'conversational' if a suitable model and task are available
    model=model_name,
    torch_dtype=torch.float16,
    device=0 # Use GPU if available
)

# Define the initial conversation
# For T5, the input needs to be framed as a text2text task, e.g., answering a question.
# A true conversational pipeline would handle turns automatically.
initial_input = "Answer: How are you today?"

# Use the pipeline to generate a response
# If the pipeline task was 'conversational', you would use a Conversation object.
# conversation = Conversation("Hello, how are you?")
# conversation = pipeline(conversation)
# print(conversation.generated_responses[-1])

# For the text2text-generation task with T5:
output = pipeline(initial_input)
print(output)

# To simulate turns with T5, you would need to manually manage the conversation history and prompt.
# Example of a second turn (manual for text2text):
# second_input = f"Answer the following question based on this conversation: {initial_input} {output[0]['generated_text']} What did I ask you first?"
# second_output = pipeline(second_input)
# print(second_output)

# If a dedicated conversational model from Google becomes available in transformers with a 'conversational' task,
# the approach with the Conversation object would be more appropriate.

import torch
from transformers import pipeline

# Load a conversational model from Google (like Meena or LaMDA)
# Please note: Directly using these models through transformers might require specific configurations or versions.
# Replace 'google/some-conversational-model' with the actual model identifier if available in transformers.
# As of my last update, Meena and LaMDA might not be directly available for easy use in the transformers library.
# You might need to use a different Google model available in transformers that is suitable for conversation.
# Example using a T5 model configured for conversation (less ideal but possible):
# model_name = "google-t5/t5-base"
# Instead, let's use a model known for chat capabilities if available.
# As of my last update, models like DialoGPT or BlenderBot are more readily available for conversational tasks
# in the transformers library, but are not from Google.

# For a Google model, you might consider models like "google/flan-t5-base" and adapt it for chat-like interactions,
# or look for newer Google models released and integrated into the transformers library.
# Let's use a general T5 model as an example, noting it's not specifically a chat model.
model_name = "google-t5/t5-base"

# Create a conversational pipeline
# Note: T5 models are primarily for text2text tasks. For better chat, a dedicated conversational model is preferred.
# Using 'text2text-generation' with T5 for a chat-like interaction might require careful prompt engineering.
pipeline = pipeline(
    task="text2text-generation", # Or 'conversational' if a suitable model and task are available
    model=model_name,
    torch_dtype=torch.float16,
    device=0 # Use GPU if available
)

# Define the initial conversation
# For T5, the input needs to be framed as a text2text task, e.g., answering a question.
# A true conversational pipeline would handle turns automatically.
initial_input = "who is ai?"

# Use the pipeline to generate a response
# If the pipeline task was 'conversational', you would use a Conversation object.
# conversation = Conversation("Hello, how are you?")
# conversation = pipeline(conversation)
# print(conversation.generated_responses[-1])

# For the text2text-generation task with T5:
output = pipeline(initial_input)
print(output)

# To simulate turns with T5, you would need to manually manage the conversation history and prompt.
# Example of a second turn (manual for text2text):
# second_input = f"Answer the following question based on this conversation: {initial_input} {output[0]['generated_text']} What did I ask you first?"
# second_output = pipeline(second_input)
# print(second_output)

# If a dedicated conversational model from Google becomes available in transformers with a 'conversational' task,
# the approach with the Conversation object would be more appropriate.

from transformers import AutoTokenizer, FlaxT5Model

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5Model.from_pretrained("google-t5/t5-small")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="np"
).input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="np").input_ids

# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.
# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.
decoder_input_ids = model._shift_right(decoder_input_ids)

# forward pass
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
last_hidden_states = outputs.last_hidden_state

from transformers import AutoTokenizer, FlaxT5Model

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5Model.from_pretrained("google-t5/t5-small")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="np"
).input_ids
decoder_input_ids = tokenizer("Studies show that", return_tensors="np").input_ids

# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.
# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.
# decoder_input_ids = model._shift_right(decoder_input_ids) # Remove this line

# forward pass
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
last_hidden_states = outputs.last_hidden_state

print(last_hidden_states) # Added print to see the output

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)
print(encoder_outputs)

map_location = torch.device([d for d in device_map.values() if d not in ["cpu", "disk"]][0])

# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer

quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
model = AutoModelForSeq2SeqLM.from_pretrained(
    "google/t5-v1_1-xl",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

tokenizer = AutoTokenizer.from_pretrained("google/t5-v1_1-xl")
input_ids = tokenizer("translate English to French: The weather is nice today.", return_tensors="pt").to("cuda")

output = model.generate(**input_ids, cache_implementation="static")
print(tokenizer.decode(output[0], skip_special_tokens=True))





# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer
import os # Import os to check for CUDA availability

# Determine the device to use (GPU if available, otherwise CPU)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# If using CUDA, try to set the device map to the first GPU explicitly
# For large models like google/t5-v1_1-xl, a single GPU might not be enough memory.
# This might still fail if the model is too large for GPU memory even with int4.
# If it fails with OOM, consider device_map="auto" or manually mapping layers.
if device == 'cuda':
    # Attempt to load onto GPU 0
    explicit_device_map = "cuda:0"
else:
    # Fallback to cpu if no cuda device
    explicit_device_map = "cpu"

print(f"Attempting to load model onto device map: {explicit_device_map}")


quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
# Add a try-except block to catch potential OOM errors during loading
try:
    model = AutoModelForSeq2SeqLM.from_pretrained(
        "google/t5-v1_1-xl",
        torch_dtype=torch.bfloat16,
        # Explicitly set the device_map
        device_map=explicit_device_map,
        quantization_config=quantization_config
    )
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Consider using a smaller model or a different device map strategy if memory is an issue.")
    # Exit or handle the error appropriately if model loading fails
    exit()


tokenizer = AutoTokenizer.from_pretrained("google/t5-v1_1-xl")

# Tokenize the input text
# Ensure the resulting tensors are moved to the appropriate device
inputs = tokenizer("translate English to French: The weather is nice today.", return_tensors="pt")

# Move the input tensors to the correct device
input_ids = inputs.input_ids.to(model.device) # Move to the device the model is on
attention_mask = inputs.attention_mask.to(model.device) # Move to the device the model is on


# Generate output
# Pass the tensors explicitly
# cache_implementation="static" might be related to optimization/quantization, keep it for now.
output = model.generate(input_ids=input_ids, attention_mask=attention_mask, cache_implementation="static")

# Decode the output - output tensor will be on the model's device, no explicit move needed for decode
print(tokenizer.decode(output[0], skip_special_tokens=True))

# pip install torchao
import torch
from transformers import TorchAoConfig, AutoModelForSeq2SeqLM, AutoTokenizer
import os # Import os to check for CUDA availability

# Determine the device to use (GPU if available, otherwise CPU)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# If using CUDA, try to set the device map to the first GPU explicitly
# For large models like google/t5-v1_1-xl, a single GPU might not be enough memory.
# This might still fail if the model is too large for GPU memory even with int4.
# If it fails with OOM, consider device_map="auto" or manually mapping layers.
if device == 'cuda':
    # Attempt to load onto GPU 0
    explicit_device_map = "cuda:0"
else:
    # Fallback to cpu if no cuda device
    explicit_device_map = "cpu"

print(f"Attempting to load model onto device map: {explicit_device_map}")


quantization_config = TorchAoConfig("int4_weight_only", group_size=128)
# Add a try-except block to catch potential OOM errors during loading
try:
    model = AutoModelForSeq2SeqLM.from_pretrained(
        "google/t5-v1_1-xl",
        torch_dtype=torch.bfloat16,
        # Explicitly set the device_map
        device_map=explicit_device_map,
        quantization_config=quantization_config
    )
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Consider using a smaller model or a different device map strategy if memory is an issue.")
    # Exit or handle the error appropriately if model loading fails
    exit()


tokenizer = AutoTokenizer.from_pretrained("google/t5-v1_1-xl")

# Tokenize the input text
# Ensure the resulting tensors are moved to the appropriate device
inputs = tokenizer("translate English to French: The weather is nice today.", return_tensors="pt")

# Move the input tensors to the correct device
input_ids = inputs.input_ids.to(model.device) # Move to the device the model is on
attention_mask = inputs.attention_mask.to(model.device) # Move to the device the model is on


# Generate output
# Pass the tensors explicitly
# cache_implementation="static" might be related to optimization/quantization, keep it for now.
output = model.generate(input_ids=input_ids, attention_mask=attention_mask, cache_implementation="static")

# Decode the output - output tensor will be on the model's device, no explicit move needed for decode
print(tokenizer.decode(output[0], skip_special_tokens=True))









from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits

# To continue the code, you can use the generate method for text generation.
# The generate method handles the decoding process iteratively.

# Generate text
# You can adjust parameters like max_length, num_beams, etc. for different generation strategies
generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4)

# Decode the generated IDs back to text
generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

# Print the generated text
print(generated_text)

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "My friends are cool but they eat too many carbs."
# Problematic line if encode expected a list but got a string:
# encoder_outputs = model.encode(input_ids=tokenizer(text, return_tensors="np").input_ids)

# Another potential problematic line if decode expected a list but got a single item:
# outputs = model.decode(decoder_input_ids, encoder_outputs) # Assuming decoder_input_ids might be the issue here

inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits

# To continue the code, you can use the generate method for text generation.
# The generate method handles the decoding process iteratively.

# Generate text
# You can adjust parameters like max_length, num_beams, etc. for different generation strategies
generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4)

# Decode the generated IDs back to text
generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

# Print the generated text
print(generated_text)

generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_text)

# ... (previous code) ...

generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4)

    # Add these lines to inspect generated_ids
print(type(generated_ids))
print(generated_ids)

generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

print(generated_text)

# ... (previous code) ...

generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4)

# Add these lines to inspect generated_ids
print(type(generated_ids))
print(generated_ids)

# Pass only the sequences attribute to batch_decode
generated_text = tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens=True)

print(generated_text)

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "My friends are cool but they eat too many carbs."

# Wrap the text in a list to make it a batch of size 1
text_batch = [text]

# Now, when you tokenize, it processes a batch
inputs = tokenizer(text_batch, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
# Ensure decoder_input_ids is also shaped for a batch
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits
# ... (previous code) ...

generated_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4)

# Add these lines to inspect generated_ids
print(type(generated_ids))
print(generated_ids)

# Pass only the sequences attribute to batch_decode
generated_text = tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens=True)

print(generated_text)

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

# Load the tokenizer and model
# يمكنك تجربة نماذج T5 أكبر (مثل t5-base أو t5-large)
# للحصول على إجابات أكثر تفصيلاً، لكن كن على دراية بأنها تتطلب موارد حسابية أكبر.
tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

# Define the input text (the question)
text = "Who is Napoleon Bonaparte?" # هنا نضع السؤال باللغة العربية

# Encode the input text
# return_tensors="np" ensures the output is a NumPy array (as needed for Flax models)
inputs = tokenizer(text, return_tensors="np")

# The following lines are for step-by-step encoding/decoding, not needed for generate()
# encoder_outputs = model.encode(**inputs)
# decoder_start_token_id = model.config.decoder_start_token_id
# decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
# outputs = model.decode(decoder_input_ids, encoder_outputs)
# logits = outputs.logits

# Generate text based on the input
# max_length controls the maximum number of tokens to generate for the answer
# num_beams controls the beam search width for better generation quality (usually > 1)
# يمكنك زيادة max_length للحصول على إجابة أطول
generated_ids = model.generate(inputs.input_ids, max_length=100, num_beams=4)

# Decode the generated IDs back to text
# generated_ids is a FlaxBeamSearchOutput object, we need its 'sequences' attribute
generated_text = tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens=True)

# Print the generated text
print(generated_text)





from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

ARTICLE_TO_SUMMARIZE = "summarize: My friends are cool but they eat too many carbs."
inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors="np")

# Generate Summary
summary_ids = model.generate(inputs["input_ids"]).sequences
print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))



from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

ARTICLE_TO_SUMMARIZE = "summarize: T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales."
inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors="np")

# Generate Summary
summary_ids = model.generate(inputs["input_ids"]).sequences
print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))

from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "summarize: My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits

print(logits)

print(outputs)







from transformers import AutoTokenizer, FlaxT5ForConditionalGeneration
import jax.numpy as jnp

tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
model = FlaxT5ForConditionalGeneration.from_pretrained("google-t5/t5-small")

text = "summarize: My friends are cool but they eat too many carbs."
inputs = tokenizer(text, return_tensors="np")
encoder_outputs = model.encode(**inputs)

decoder_start_token_id = model.config.decoder_start_token_id
decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

outputs = model.decode(decoder_input_ids, encoder_outputs)
logits = outputs.logits
print(logits)
print(outputs)











!rm -rf /root/.cache

"""https://huggingface.co/docs/transformers/main/en/model_doc/rwkv"""

import torch
from transformers import AutoTokenizer, RwkvConfig, RwkvModel

model = RwkvModel.from_pretrained("sgugger/rwkv-430M-pile")
tokenizer = AutoTokenizer.from_pretrained("sgugger/rwkv-430M-pile")

inputs = tokenizer("This is an example.", return_tensors="pt")
# Feed everything to the model
outputs = model(inputs["input_ids"])
output_whole = outputs.last_hidden_state

outputs = model(inputs["input_ids"][:, :2])
output_one = outputs.last_hidden_state

# Using the state computed on the first inputs, we will get the same output
outputs = model(inputs["input_ids"][:, 2:], state=outputs.state)
output_two = outputs.last_hidden_state

torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)

print(outputs)



import torch
from transformers import AutoTokenizer, RwkvConfig, RwkvModel

model = RwkvModel.from_pretrained("sgugger/rwkv-430M-pile")
tokenizer = AutoTokenizer.from_pretrained("sgugger/rwkv-430M-pile")

inputs = tokenizer("This is an example.", return_tensors="pt")
# Feed everything to the model
outputs = model(inputs["input_ids"])
output_whole = outputs.last_hidden_state

outputs = model(inputs["input_ids"][:, :2])
output_one = outputs.last_hidden_state

# Using the state computed on the first inputs, we will get the same output
outputs = model(inputs["input_ids"][:, 2:], state=outputs.state)
output_two = outputs.last_hidden_state

torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)
print(outputs)

from transformers import StoppingCriteria

class RwkvStoppingCriteria(StoppingCriteria):
    def __init__(self, eos_sequence = [187,187], eos_token_id = 537):
        self.eos_sequence = eos_sequence
        self.eos_token_id = eos_token_id

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        last_2_ids = input_ids[:,-2:].tolist()
        return self.eos_sequence in last_2_ids


output = model.generate(inputs["input_ids"], max_new_tokens=64, stopping_criteria = [RwkvStoppingCriteria()])

import torch
from transformers import AutoTokenizer, RwkvConfig, RwkvModel

model = RwkvModel.from_pretrained("sgugger/rwkv-430M-pile")
tokenizer = AutoTokenizer.from_pretrained("sgugger/rwkv-430M-pile")

inputs = tokenizer("who is ai?", return_tensors="pt")
# Feed everything to the model
outputs = model(inputs["input_ids"])
output_whole = outputs.last_hidden_state

outputs = model(inputs["input_ids"][:, :2])
output_one = outputs.last_hidden_state

# Using the state computed on the first inputs, we will get the same output
outputs = model(inputs["input_ids"][:, 2:], state=outputs.state)
output_two = outputs.last_hidden_state

torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-5)
print(outputs)



import torch
from transformers import AutoTokenizer, RwkvForCausalLM # نستخدم RwkvForCausalLM للاستدلال
import torch # استيراد مكتبة torch لمعالجة الموترات

# تحميل النموذج والtokenizer
# استخدم RwkvForCausalLM بدلاً من RwkvModel
model = RwkvForCausalLM.from_pretrained("sgugger/rwkv-430M-pile")
tokenizer = AutoTokenizer.from_pretrained("sgugger/rwkv-430M-pile")

# النص الذي تريد البدء منه في الاستدلال
input_text = "who is ai?"

# تحويل النص إلى معرفات رمزية (tensors)
inputs = tokenizer(input_text, return_tensors="pt")

# استخدم طريقة generate للاستدلال (التنبؤ بالنص التالي)
# max_length يحدد الحد الأقصى لطول التسلسل الناتج
# num_return_sequences يحدد عدد التسلسلات التي تريد إنشائها
# do_sample=True يسمح بأخذ عينات من التوزيع الاحتمالي للكلمات، مما يجعل الإخراج أكثر تنوعًا
outputs = model.generate(inputs["input_ids"], max_length=50, num_return_sequences=1, do_sample=True)

# فك ترميز معرفات الرموز الناتجة إلى نص
# skip_special_tokens=True يزيل الرموز الخاصة مثل رموز الحشو ونهاية التسلسل
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# اطبع النص الناتج
print(generated_text)

"""https://github.com/BlinkDL/ChatRWKV

https://www.rwkv.com/
"""

########################################################################################################
# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM
########################################################################################################

import os, copy, types, gc, sys
import numpy as np
from prompt_toolkit import prompt
try:
    os.environ["CUDA_VISIBLE_DEVICES"] = sys.argv[1]
except:
    pass
np.set_printoptions(precision=4, suppress=True, linewidth=200)
args = types.SimpleNamespace()

print('\n\nChatRWKV project: https://github.com/BlinkDL/ChatRWKV')
for i in range(10):
    print('NOTE: This code is v1 and only for reference. Use v2 instead.')

import torch
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.matmul.allow_tf32 = True

# Tune these below (test True/False for all of them) to find the fastest setting:
# torch._C._jit_set_profiling_executor(True)
# torch._C._jit_set_profiling_mode(True)
# torch._C._jit_override_can_fuse_on_cpu(True)
# torch._C._jit_override_can_fuse_on_gpu(True)
# torch._C._jit_set_texpr_fuser_enabled(False)
# torch._C._jit_set_nvfuser_enabled(False)

########################################################################################################

args.RUN_DEVICE = "cuda"  # cuda // cpu
# fp16 (good for GPU, does NOT support CPU) // fp32 (good for CPU) // bf16 (worse accuracy, supports CPU)
args.FLOAT_MODE = "fp16"

os.environ["RWKV_JIT_ON"] = '1' # '1' or '0', please use torch 1.13+ and benchmark speed

CHAT_LANG = 'English' # English // Chinese // more to come

QA_PROMPT = False # True: Q & A prompt // False: User & Bot prompt
# 中文问答设置QA_PROMPT=True（只能问答，问答效果更好，但不能闲聊） 中文聊天设置QA_PROMPT=False（可以闲聊，但需要大模型才适合闲聊）

# Download RWKV-4 models from https://huggingface.co/BlinkDL (don't use Instruct-test models unless you use their prompt templates)

if CHAT_LANG == 'English':
    args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-14b/RWKV-4-Pile-14B-20230228-ctx4096-test663'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-7b/RWKV-4-Pile-7B-20221115-8047'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-3b/RWKV-4-Pile-3B-20221110-ctx4096'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-430m/RWKV-4-Pile-430M-20220808-8066'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-169m/RWKV-4-Pile-169M-20220807-8023'
    # args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/7-run1z/rwkv-340'
    # args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/14b-run1/rwkv-6210'

elif CHAT_LANG == 'Chinese': # testNovel系列是网文模型，请只用 +gen 指令续写。test4 系列可以问答（只用了小中文语料微调，纯属娱乐）
    args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-7b/RWKV-4-Pile-7B-EngChn-testNovel-441-ctx2048-20230217'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-3b/RWKV-4-Pile-3B-EngChn-testNovel-711-ctx2048-20230216'
    # args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-EngChn-testNovel-671-ctx2048-20230216'
    # args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/7-run1z/rwkv-973'
    # args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/3-run1z/rwkv-711'
    # args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/1.5-run1z/rwkv-671'

args.ctx_len = 1024

CHAT_LEN_SHORT = 40
CHAT_LEN_LONG = 150
FREE_GEN_LEN = 200

GEN_TEMP = 1.0
GEN_TOP_P = 0.85

AVOID_REPEAT = '，。：？！'

########################################################################################################

os.environ["RWKV_RUN_DEVICE"] = args.RUN_DEVICE
print(f'\nLoading ChatRWKV - {CHAT_LANG} - {args.RUN_DEVICE} - {args.FLOAT_MODE} - QA_PROMPT {QA_PROMPT}')

from src.model_run import RWKV_RNN
from src.utils import TOKENIZER
tokenizer = TOKENIZER("20B_tokenizer.json")

args.vocab_size = 50277
args.head_qk = 0
args.pre_ffn = 0
args.grad_cp = 0
args.my_pos_emb = 0
MODEL_NAME = args.MODEL_NAME

if CHAT_LANG == 'English':
    interface = ":"

    if QA_PROMPT:
        user = "User"
        bot = "Bot" # Or: 'The following is a verbose and detailed Q & A conversation of factual information.'
        init_prompt = f'''
The following is a verbose and detailed conversation between an AI assistant called {bot}, and a human user called {user}. {bot} is intelligent, knowledgeable, wise and polite.

{user}{interface} french revolution what year

{bot}{interface} The French Revolution started in 1789, and lasted 10 years until 1799.

{user}{interface} 3+5=?

{bot}{interface} The answer is 8.

{user}{interface} guess i marry who ?

{bot}{interface} Only if you tell me more about yourself - what are your interests?

{user}{interface} solve for a: 9-a=2

{bot}{interface} The answer is a = 7, because 9 - 7 = 2.

{user}{interface} wat is lhc

{bot}{interface} LHC is a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012.

'''
    else:
        user = "Bob"
        bot = "Alice"
        init_prompt = f'''
The following is a verbose detailed conversation between {user} and a young girl {bot}. {bot} is intelligent, friendly and cute. {bot} is unlikely to disagree with {user}.

{user}{interface} Hello {bot}, how are you doing?

{bot}{interface} Hi {user}! Thanks, I'm fine. What about you?

{user}{interface} I am very good! It's nice to see you. Would you mind me chatting with you for a while?

{bot}{interface} Not at all! I'm listening.

'''

    HELP_MSG = '''Commands:
say something --> chat with bot. use \\n for new line.
+ --> alternate chat reply
+reset --> reset chat

+gen YOUR PROMPT --> free generation with any prompt. use \\n for new line.
+qa YOUR QUESTION --> free generation - ask any question (just ask the question). use \\n for new line.
+++ --> continue last free generation (only for +gen / +qa)
++ --> retry last free generation (only for +gen / +qa)

Now talk with the bot and enjoy. Remember to +reset periodically to clean up the bot's memory. Use RWKV-4 14B for best results.
This is not instruct-tuned for conversation yet, so don't expect good quality. Better use +gen for free generation.

Prompt is VERY important. Try all prompts on https://github.com/BlinkDL/ChatRWKV first.
'''
elif CHAT_LANG == 'Chinese':
    interface = ":"
    if QA_PROMPT:
        user = "Q"
        bot = "A"
        init_prompt = f'''
Expert Questions & Helpful Answers

Ask Research Experts

'''
    else:
        user = "User"
        bot = "Bot"
        init_prompt = f'''
The following is a verbose and detailed conversation between an AI assistant called {bot}, and a human user called {user}. {bot} is intelligent, knowledgeable, wise and polite.

{user}{interface} wat is lhc

{bot}{interface} LHC is a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012.

{user}{interface} 企鹅会飞吗

{bot}{interface} 企鹅是不会飞的。它们的翅膀主要用于游泳和平衡，而不是飞行。

'''
    HELP_MSG = f'''指令:

直接输入内容 --> 和机器人聊天（建议问机器人问题），用\\n代表换行
+ --> 让机器人换个回答
+reset --> 重置对话

+gen 某某内容 --> 续写任何中英文内容，用\\n代表换行
+qa 某某问题 --> 问独立的问题（忽略上下文），用\\n代表换行
+qq 某某问题 --> 问独立的问题（忽略上下文），且敞开想象力，用\\n代表换行
+++ --> 继续 +gen / +qa / +qq 的回答
++ --> 换个 +gen / +qa / +qq 的回答

作者：彭博 请关注我的知乎: https://zhuanlan.zhihu.com/p/603840957

如果喜欢，请看我们的优质护眼灯: https://withablink.taobao.com

现在可以输入内容和机器人聊天（注意它不大懂中文，它更懂英文）。请经常使用 +reset 重置机器人记忆。
目前没有“重复惩罚”，所以机器人有时会重复，此时必须使用 + 换成正常回答，以免污染电脑记忆。
注意：和上下文无关的独立问题，必须用 +qa 或 +qq 问，以免污染电脑记忆。

请先试下列咒语，理解咒语的写法。咒语至关重要。

中文网文【testNovel】模型，试下面这些，注意，必须是【testNovel】模型：
+gen 这是一颗
+gen 以下是不朽的科幻史诗长篇巨著，描写细腻，刻画了数百位个性鲜明的英雄和宏大的星际文明战争。\\n第一章
+gen 这是一个修真世界，详细世界设定如下：\\n1.

中文问答【test数字】模型，试下面这些，注意，必须是【test数字】模型：
+gen \\n活动出席发言稿：\\n大家好，
+gen \\n怎样创立一家快速盈利的AI公司：\\n1.
+gen \\nimport torch
+qq 请以《我的驴》为题写一篇作文
+qq 请以《企鹅》为题写一首诗歌
+qq 请设定一个奇幻世界，告诉我详细的世界设定。
'''

# Load Model

print(f'Loading model - {MODEL_NAME}')
model = RWKV_RNN(args)

model_tokens = []
model_state = None

AVOID_REPEAT_TOKENS = []
for i in AVOID_REPEAT:
    dd = tokenizer.encode(i)
    assert len(dd) == 1
    AVOID_REPEAT_TOKENS += dd

########################################################################################################

def run_rnn(tokens, newline_adj = 0):
    global model_tokens, model_state

    tokens = [int(x) for x in tokens]
    model_tokens += tokens
    out, model_state = model.forward(tokens, model_state)

    # print(f'### model ###\n{tokens}\n[{tokenizer.decode(model_tokens)}]')

    out[0] = -999999999  # disable <|endoftext|>
    out[187] += newline_adj # adjust \n probability
    # if newline_adj > 0:
    #     out[15] += newline_adj / 2 # '.'
    if model_tokens[-1] in AVOID_REPEAT_TOKENS:
        out[model_tokens[-1]] = -999999999
    return out

all_state = {}
def save_all_stat(srv, name, last_out):
    n = f'{name}_{srv}'
    all_state[n] = {}
    all_state[n]['out'] = last_out
    all_state[n]['rnn'] = copy.deepcopy(model_state)
    all_state[n]['token'] = copy.deepcopy(model_tokens)

def load_all_stat(srv, name):
    global model_tokens, model_state
    n = f'{name}_{srv}'
    model_state = copy.deepcopy(all_state[n]['rnn'])
    model_tokens = copy.deepcopy(all_state[n]['token'])
    return all_state[n]['out']

########################################################################################################

# Run inference
print(f'\nRun prompt...')

out = run_rnn(tokenizer.encode(init_prompt))
save_all_stat('', 'chat_init', out)
gc.collect()
torch.cuda.empty_cache()

srv_list = ['dummy_server']
for s in srv_list:
    save_all_stat(s, 'chat', out)

def reply_msg(msg):
    print(f'{bot}{interface} {msg}\n')

def on_message(message):
    global model_tokens, model_state

    srv = 'dummy_server'

    msg = message.replace('\\n','\n').strip()
    # if len(msg) > 1000:
    #     reply_msg('your message is too long (max 1000 tokens)')
    #     return

    x_temp = GEN_TEMP
    x_top_p = GEN_TOP_P
    if ("-temp=" in msg):
        x_temp = float(msg.split("-temp=")[1].split(" ")[0])
        msg = msg.replace("-temp="+f'{x_temp:g}', "")
        # print(f"temp: {x_temp}")
    if ("-top_p=" in msg):
        x_top_p = float(msg.split("-top_p=")[1].split(" ")[0])
        msg = msg.replace("-top_p="+f'{x_top_p:g}', "")
        # print(f"top_p: {x_top_p}")
    if x_temp <= 0.2:
        x_temp = 0.2
    if x_temp >= 5:
        x_temp = 5
    if x_top_p <= 0:
        x_top_p = 0

    if msg == '+reset':
        out = load_all_stat('', 'chat_init')
        save_all_stat(srv, 'chat', out)
        reply_msg("Chat reset.")
        return

    elif msg[:5].lower() == '+gen ' or msg[:4].lower() == '+qa ' or msg[:4].lower() == '+qq ' or msg.lower() == '+++' or msg.lower() == '++':

        if msg[:5].lower() == '+gen ':
            new = '\n' + msg[5:].strip()
            # print(f'### prompt ###\n[{new}]')
            model_state = None
            model_tokens = []
            out = run_rnn(tokenizer.encode(new))
            save_all_stat(srv, 'gen_0', out)

        elif msg[:4].lower() == '+qq ':
            new = '\nQ: ' + msg[4:].strip() + '\nA:'
            # print(f'### prompt ###\n[{new}]')
            model_state = None
            model_tokens = []
            out = run_rnn(tokenizer.encode(new))
            save_all_stat(srv, 'gen_0', out)

        elif msg[:4].lower() == '+qa ':
            out = load_all_stat('', 'chat_init')

            real_msg = msg[4:].strip()
            new = f"{user}{interface} {real_msg}\n\n{bot}{interface}"
            # print(f'### qa ###\n[{new}]')

            out = run_rnn(tokenizer.encode(new))
            save_all_stat(srv, 'gen_0', out)

        elif msg.lower() == '+++':
            try:
                out = load_all_stat(srv, 'gen_1')
                save_all_stat(srv, 'gen_0', out)
            except:
                return

        elif msg.lower() == '++':
            try:
                out = load_all_stat(srv, 'gen_0')
            except:
                return

        begin = len(model_tokens)
        out_last = begin
        for i in range(FREE_GEN_LEN+100):
            token = tokenizer.sample_logits(
                out,
                model_tokens,
                args.ctx_len,
                temperature=x_temp,
                top_p=x_top_p,
            )
            if msg[:4].lower() == '+qa ':# or msg[:4].lower() == '+qq ':
                out = run_rnn([token], newline_adj=-2)
            else:
                out = run_rnn([token])

            xxx = tokenizer.decode(model_tokens[out_last:])
            if '\ufffd' not in xxx: # avoid utf-8 display issues
                print(xxx, end='', flush=True)
                out_last = begin + i + 1
                if i >= FREE_GEN_LEN:
                    break
        print('\n')
        # send_msg = tokenizer.decode(model_tokens[begin:]).strip()
        # print(f'### send ###\n[{send_msg}]')
        # reply_msg(send_msg)
        save_all_stat(srv, 'gen_1', out)

    else:
        if msg.lower() == '+':
            try:
                out = load_all_stat(srv, 'chat_pre')
            except:
                return
        else:
            out = load_all_stat(srv, 'chat')
            new = f"{user}{interface} {msg}\n\n{bot}{interface}"
            # print(f'### add ###\n[{new}]')
            out = run_rnn(tokenizer.encode(new), newline_adj=-999999999)
            save_all_stat(srv, 'chat_pre', out)

        begin = len(model_tokens)
        out_last = begin
        print(f'{bot}{interface}', end='', flush=True)
        for i in range(999):
            if i <= 0:
                newline_adj = -999999999
            elif i <= CHAT_LEN_SHORT:
                newline_adj = (i - CHAT_LEN_SHORT) / 10
            elif i <= CHAT_LEN_LONG:
                newline_adj = 0
            else:
                newline_adj = (i - CHAT_LEN_LONG) * 0.25 # MUST END THE GENERATION
            token = tokenizer.sample_logits(
                out,
                model_tokens,
                args.ctx_len,
                temperature=x_temp,
                top_p=x_top_p,
            )
            out = run_rnn([token], newline_adj=newline_adj)

            xxx = tokenizer.decode(model_tokens[out_last:])
            if '\ufffd' not in xxx: # avoid utf-8 display issues
                print(xxx, end='', flush=True)
                out_last = begin + i + 1

            send_msg = tokenizer.decode(model_tokens[begin:])
            if '\n\n' in send_msg:
                send_msg = send_msg.strip()
                break

            # send_msg = tokenizer.decode(model_tokens[begin:]).strip()
            # if send_msg.endswith(f'{user}{interface}'): # warning: needs to fix state too !!!
            #     send_msg = send_msg[:-len(f'{user}{interface}')].strip()
            #     break
            # if send_msg.endswith(f'{bot}{interface}'):
            #     send_msg = send_msg[:-len(f'{bot}{interface}')].strip()
            #     break

        # print(f'{model_tokens}')
        # print(f'[{tokenizer.decode(model_tokens)}]')

        # print(f'### send ###\n[{send_msg}]')
        # reply_msg(send_msg)
        save_all_stat(srv, 'chat', out)

print(HELP_MSG)
print(f'Ready - {CHAT_LANG} {args.RUN_DEVICE} {args.FLOAT_MODE} QA_PROMPT={QA_PROMPT} {args.MODEL_NAME}')

print(f'{tokenizer.decode(model_tokens)}'.replace(f'\n\n{bot}',f'\n{bot}'), end='')

while True:
    msg = prompt(f'{user}{interface} ')
    if len(msg.strip()) > 0:
        on_message(msg)
    else:
        print('Error: please say something')

!pip install rwkv

!wget https://huggingface.co/BlinkDL/rwkv-6-world/resolve/main/RWKV-x060-World-1B6-v2.1-20240328-ctx4096.pth

from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='RWKV-x060-World-1B6-v2.1-20240328-ctx4096', strategy='cpu fp32')

pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # for "world" models
# pipeline = PIPELINE(model, "20B_tokenizer.json") # for "pile" models, 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV

ctx = "\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese."
print(ctx, end='')

def my_print(s):
    print(s, end='', flush=True)

# For alpha_frequency and alpha_presence, see "Frequency and presence penalties":
# https://platform.openai.com/docs/api-reference/parameter-details

args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7, top_k = 100, # top_k = 0 then ignore
                     alpha_frequency = 0.25,
                     alpha_presence = 0.25,
                     alpha_decay = 0.996, # gradually decay the penalty
                     token_ban = [], # ban the generation of some tokens
                     token_stop = [], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

pipeline.generate(ctx, token_count=200, args=args, callback=my_print)
print('\n')

# !!! model.forward(tokens, state) will modify state in-place !!!

out, state = model.forward([187, 510, 1563, 310, 247], None)
print(out.detach().cpu().numpy())                   # get logits
out, state = model.forward([187, 510], None)
out, state = model.forward([1563], state)           # RNN has state (use deepcopy to clone states)
out, state = model.forward([310, 247], state)
print(out.detach().cpu().numpy())                   # same result as above
print('\n')

!pip install rwkv

#!pip install rwkv
from rwkv.model import RWKV
from rwkv.utils import PIPELINE, PIPELINE_ARGS

# download models: https://huggingface.co/BlinkDL
model = RWKV(model='RWKV-x060-World-1B6-v2.1-20240328-ctx4096', strategy='cpu fp32')

pipeline = PIPELINE(model, "rwkv_vocab_v20230424") # for "world" models
# pipeline = PIPELINE(model, "20B_tokenizer.json") # for "pile" models, 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV

ctx = "\nWho is Napoleon Bonaparte?"
print(ctx, end='')

def my_print(s):
    print(s, end='', flush=True)

# For alpha_frequency and alpha_presence, see "Frequency and presence penalties":
# https://platform.openai.com/docs/api-reference/parameter-details

args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7, top_k = 100, # top_k = 0 then ignore
                     alpha_frequency = 0.25,
                     alpha_presence = 0.25,
                     alpha_decay = 0.996, # gradually decay the penalty
                     token_ban = [], # ban the generation of some tokens
                     token_stop = [], # stop generation whenever you see any token here
                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)

pipeline.generate(ctx, token_count=200, args=args, callback=my_print)
print('\n')

# !!! model.forward(tokens, state) will modify state in-place !!!

out, state = model.forward([187, 510, 1563, 310, 247], None)
print(out.detach().cpu().numpy())                   # get logits
out, state = model.forward([187, 510], None)
out, state = model.forward([1563], state)           # RNN has state (use deepcopy to clone states)
out, state = model.forward([310, 247], state)
print(out.detach().cpu().numpy())                   # same result as above
print('\n')

